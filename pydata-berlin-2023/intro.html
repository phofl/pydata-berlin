  <!--


-->

<!DOCTYPE html>
<html>
  <head>
    <title>pandas 2.0 and beyond</title>
    <meta charset="utf-8">
      <link rel="stylesheet" type="text/css" href="slides.css">
      <link rel="stylesheet" type="text/css" href="abs-layout.css" />
      <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css" />

<!--    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono';   }
      #slideshow .slide .content .cols.two .col { width: 48%; }
    </style>
-->
  </head>
  <body>
    <textarea id="source">

class: center, middle

![:scale 40%](img/pandas.svg)

# pandas 2.0 and beyond

### Copy-on-Write, Arrow-backed DataFrames and more

Joris Van den Bossche (Voltron Data), Patrick Hoefler (Coiled)<br>
PyConDE & PyData Berlin, April 17, 2023

https://github.com/jorisvandenbossche/<br>
[@jorisvdbossche](https://twitter.com/jorisvdbossche)/<br>
https://github.com/phofl/<br>


---

# About Joris

Joris Van den Bossche
<div style="margin-bottom:-20px"></div>

- Background: PhD bio-science engineer, air quality research
- Open source enthusiast: pandas core dev, geopandas maintainer, scikit-learn contributor
- Currently working part-time at Voltron Data on Apache Arrow

https://github.com/jorisvandenbossche   Twitter: [@jorisvdbossche](https://twitter.com/jorisvdbossche)


.affiliations[
  ![:scale 30%](img/apache-arrow.png)
  ![:scale 64%](img/voltrondata-logo-green.png)
]

---

# About Patrick

Patrick Hoefler
<div style="margin-bottom:-20px"></div>

- Background: M.Sc. in Mathematics
- Open source: pandas core dev
- Currently working at Coiled on Dask

https://github.com/phofl


.affiliations[
  ![:scale 20%](img/dask.png)
  ![:scale 30%](img/coiled.png)
]
---

class: center, middle

# pandas 2.0 and beyond

### Copy-on-Write, Arrow-backed DataFrames and more


---
class: center, middle

# PDEP-7: Consistent copy/view semantics in pandas with Copy-on-Write

### a.k.a. Getting rid of the SettingWithCopyWarning

---

# Current situation: SettingWithCopyWarning

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
>>> subset = df[df["A"] > 1]
>>> subset.loc[1, "A"] = 10
# SettingWithCopyWarning: 
# A value is trying to be set on a copy of a slice from a DataFrame.
# Try using .loc[row_indexer,col_indexer] = value instead
#
# See the caveats in the documentation: ...
```

---

# Current situation: copy vs view

.center[
![:scale 80%](img/view-vs-copy.png)
]

Images from https://www.dataquest.io/blog/settingwithcopywarning/

---

# Current situation: copy vs view

.center[
![:scale 80%](img/view-vs-copy-modifying.png)
]

Images from https://www.dataquest.io/blog/settingwithcopywarning/

---

# Current situation

Problems with the current copy / view semantics of pandas:

- This is confusing for many users
- You need to be aware of copy/view details of numpy
- You need defensive (and unncecessary) copying to avoid the warning


???

Copies and views in NumPy

explain a view in numpy

slicing gives view, mask gives copy


---

# The SettingWithCopyWarning

To avoid "chained assignment (setitem)" pitfalls:

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
>>> df["C"][df["A"] > 1] = 10  # this works
>>> df[df["A"] > 1]["C"] = 10  # this doesn't work
```

--
count: false

Rewriting the second case:

```python
>>> temp = df[df["A"] > 1]
>>> temp["C"] = 10
```

--
count: false

Our previous example:

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
>>> subset = df[df["A"] > 1]
...
>>> subset["C"] = 10
```

---

# The SettingWithCopyWarning

How to "solve" the warning?

--
count: false

I want do update `df` -> setitem in one go

```python
>>> df[df["A"] > 1]["C"] = 10      # this doesn't work
>>> df`.loc[df["A"] > 1, "C"]` = 10  # this works
```

Or use `.assign(A=...)` method instead.

--
count: false

I don't want to update `df` -> explicit (unnecessary) `copy()`

```python
>>> subset = df[df["A"] > 1]`.copy()`
...
>>> subset["C"] = 10
```

---

# Current situation

Problems with the current copy / view semantics of pandas:

- This is confusing for many users
- You need to be aware of copy/view details of numpy
- **You need defensive (and unncecessary) copying to avoid the warning**

---

## Intermezzo: copies and method chaining

Which steps of this workflow copy the DataFrame?

```python
(df
  .rename(columns=lambda col: col.replace(".", "_"))
  .assign(team_size=lambda df_: df_.team_size.replace(...))
  .astype({'employment_status': "category", ...})
  .drop(columns=[...])
  .dropna(subset="year")
  .reset_index()
)
```

---
count: false

## Intermezzo: copies and method chaining

Which steps of this workflow copy the DataFrame?

```python
(df
* .rename(columns=lambda col: col.replace(".", "_"))
* .assign(team_size=lambda df_: df_.team_size.replace(...))
* .astype({'employment_status': "category", ...})
* .drop(columns=[...])
  .dropna(subset="year")
* .reset_index()
)
```

--
count: false

The `inplace` keyword *sometimes* avoids the copy, but is not recommended.

---

# Can we do better?

A proposal for simplified behaviour using a single rule:

> *Any DataFrame or Series derived from another in any way (e.g. with an indexing operation) always *behaves as* a copy.*

--
count: false

Or put differently, the implication is:

> *Mutating a DataFrame only changes the object itself, and not any other.*

> *If you want to change values in a DataFrame or Series, you can only do that by directly mutating the DataFrame/Series at hand*.


---

# Can we do better?

A proposal for simplified behaviour using a single rule:

> *Any DataFrame or Series derived from another in any way (e.g. with an indexing operation) always *behaves as* a copy.*

Advantages:

- A simpler, more consistent user experience
- We can get rid of the SettingWithCopyWarning (since there is no confusion about whether we are mutating a view or a copy)
- We would no longer need defensive copying in many places in pandas, improving memory usage

---
count: false

# Can we do better?

A proposal for simplified behaviour using a single rule:

> *Any DataFrame or Series derived from another in any way (e.g. with an indexing operation) always *behaves as* a copy.*

Advantages:

- **A simpler, more consistent user experience**
- We can get rid of the SettingWithCopyWarning (since there is no confusion about whether we are mutating a view or a copy)
- We would no longer need defensive copying in many places in pandas, improving memory usage

---

## Examples: filtering rows with a boolean mask

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
*>>> subset = df[df["A"] > 1]
>>> subset.loc[1, "A"] = 10
```

Did `df` change as well?

--
count: false

No, `subset` is a different object, so mutating `subset` does not change `df`.

---

## Examples: slicing rows

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
*>>> subset = df[1:2]
>>> subset.loc[1, "A"] = 10
```

Did `df` change as well?

--
count: false

No, `subset` is a different object, so mutating `subset` does not change `df`.

---

## Examples: subsetting columns

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
*>>> subset = df[["A", "B"]]
>>> subset.loc[1, "A"] = 10
```

Did `df` change as well?

--
count: false

No, `subset` is a different object, so mutating `subset` does not change `df`.

---

## Examples: selecting a single column

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
*>>> subset = df["A"]
>>> subset[1] = 10
```

Did `df` change as well?

--
count: false

No, `subset` is a different object, so mutating `subset` does not change `df`.

---

# Can we do better?

A proposal for simplified behaviour using a single rule:

> *Any DataFrame or Series derived from another in any way (e.g. with an indexing operation) always *behaves as* a copy.*

Advantages:

- A simpler, more consistent user experience
- **We can get rid of the SettingWithCopyWarning** (since there is no confusion about whether we are mutating a view or a copy)
- We would no longer need defensive copying in many places in pandas, improving memory usage

---

# The SettingWithCopyWarning

With current pandas:

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
# two examples of chained assignment
>>> df["C"][df["A"] > 1] = 10  # this works
>>> df[df["A"] > 1]["C"] = 10  # this doesn't work
```

--
count: false

With proposal: both examples don't work

--
count: false

```python
>>> temp = df["C"]
>>> temp[temp["A"] > 1] = 10
```

Chained assignment will never work -> we don't need a warning.

---

# The SettingWithCopyWarning

With current pandas:

```python
>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4], "C": [5, 6]})
>>> subset = df[df["A"] > 1]`.copy()`
...
>>> subset["C"] = 10
```

With proposal: additional `copy()` is no longer needed to avoid the warning.

---

# Can we do better?

A proposal for simplified behaviour using a single rule:

> *Any DataFrame or Series derived from another in any way (e.g. with an indexing operation) always *<span class="remark-code-span-highlighted">behaves as</span>* a copy.*

Advantages:

- A simpler, more consistent user experience
- We can get rid of the SettingWithCopyWarning (since there is no confusion about whether we are mutating a view or a copy)
- **We would no longer need defensive copying in many places in pandas, improving memory usage**

---

## Hiding copy/view details with Copy-on-Write

Guarantee == "behaves as a copy"

Copy-on-Write: usage of views becomes internal implementation detail

-> We can avoid copies by default using Copy-on-Write!

--

**Small benchmark**: create DataFrame of 2 million rows and 30 columns (mix of float, ints and strings)
```python
import pandas as pd
import numpy as np

N = 2_000_000
int_df = pd.DataFrame(np.random.randint(1, 100, (N, 10)), columns=[f"col_{i}" for i in range(10)])
float_df = pd.DataFrame(np.random.random((N, 10)), columns=[f"col_{i}" for i in range(10, 20)])
str_df = pd.DataFrame("a", index=range(N), columns=[f"col_{i}" for i in range(20, 30)])

df = pd.concat([int_df, float_df, str_df], axis=1)
```

---

## Hiding copy/view details with Copy-on-Write

Guarantee == "behaves as a copy"

Copy-on-Write: usage of views becomes internal implementation detail

-> We can avoid copies by default using Copy-on-Write!

```python
%%timeit
(df.rename(columns={"col_1": "new_index"})
   .assign(sum_val=df["col_1"] + df["col_2"])
   .drop(columns=["col_10", "col_20"])
   .astype({"col_5": "int32"})
   .reset_index()
   .set_index("new_index")
)
```

```
2.45 s ± 293 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
--
count: false

```
# with Copy-on-Write enabled
*13.7 ms ± 286 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

---

## Hiding copy/view details with Copy-on-Write

When an operations makes an actual copy of the data (e.g. `df2 = df1.copy()`)

.center[
![:scale 60%](img/copy-on-write_copy.svg)
]

---
## Hiding copy/view details with Copy-on-Write

When an operations gives a view (e.g. `df2 = df1.reset_index()`)

.center[
![:scale 60%](img/copy-on-write_view.svg)
]


---
## Hiding copy/view details with Copy-on-Write

Modifying a view or its parent (`df1` or `df2`) will trigger a copy (a "copy on write"), and each DataFrame now owns its own memory.

.center[
![:scale 60%](img/copy-on-write_modified.svg)
]


---
## Hiding copy/view details with Copy-on-Write

Do you want to avoid this copy when modifying `df2`, and no longer need `df1`? 
You can for example reassign to the same variable (`df1 = df1.reset_index()`, such that the original `df1` goes out of scope.

.center[
![:scale 60%](img/copy-on-write_reassign.svg)
]


---

# Can we do better?

A proposal for simplified behaviour using a single rule:

> *Any DataFrame or Series derived from another in any way (e.g. with an indexing operation) always *behaves as* a copy.*

Advantages:

- A simpler, more consistent user experience
- We can get rid of the SettingWithCopyWarning (since there is no confusion about whether we are mutating a view or a copy)
- We would no longer need defensive copying in many places in pandas, improving memory usage

---

# How do I try this?

Enable it in pandas 2.0:

```
import pandas as pd
pd.options.mode.copy_on_write = True
```

More details: 

* Blogpost: https://jorisvandenbossche.github.io/blog/2022/04/07/pandas-copy-views/
* Full proposal: https://docs.google.com/document/d/1ZCQ9mx3LBMy-nhwRl33_jgcvWo9IWdEfxDNQ2thyTb0/edit#heading=h.iexejdstiz8u
* Related GitHub issue: https://github.com/pandas-dev/pandas/issues/36195


## Feedback welcome!


---

count: false

# Arrow-backed DataFrames

Using PyArrow arrays to store the data of a DataFrame.


---

## ArrowDtype

`pd.ArrowDtype` or `f"{dtype}[pyarrow]"` creates Arrow-backed columns

--
count: false

```python
>>> import pyarrow as pa
>>> pd.Series([1, 2, 3], dtype=pd.ArrowDtype(pa.int64()))
0   1
1   2
2   3
dtype: int64[pyarrow]

>>> pd.Series([1, 2, 3], dtype="int64[pyarrow]")
0   1
1   2
2   3
dtype: int64[pyarrow]
```

--
count: false

These columns use the PyArrow memory layout and compute functionality.

---

## Dispatching to pyarrow.compute

The ExtensionArray interface of pandas dispatches to
[compute functions of PyArrow](https://arrow.apache.org/docs/python/compute.html).

```python
In [3]: ser = pd.Series(np.random.randint(1, 100, (5_000_000, )))
```
--
count: false
```python
In [4]: %timeit ser.unique()
10.6 ms ± 31.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
--
count: false

```python
In [5]: ser_arrow = ser.astype(pd.ArrowDtype(pa.int64()))

In [6]: %timeit ser_arrow.unique()
6.71 ms ± 6.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

--
count: false

.larger[&rarr;] PyArrow can provide a significant performance improvement

Not every method of pandas supports the compute functionality of PyArrow yet.

---

## PyArrow dtypes

PyArrow offers support for a wide variety of dtypes.

--
count: false

.larger[Benefits]

- Support of missing value indicators in every datatype

```python
>>> pd.Series([1, None], dtype="int64[pyarrow]")
0       1
1    <NA>
dtype: int64[pyarrow]
```
--
count: false
- An efficient string-datatype implementation

--
count: false

- Bytes, decimal, explicit null-datatype and [many more](https://arrow.apache.org/docs/python/api/datatypes.html#data-types-and-schemas).

---


## PyArrow string dtype

PyArrow offers fast and efficient in-memory string operations.

pandas implements PyArrow-based string operations through `"string[pyarrow]"` or `pd.ArrowDtype(pa.string())`.

--
count: false

These implementations provide

- significantly improved performance compared to NumPy's object dtype
- smaller memory footprint

--
count: false

If the functionality does not exist in PyArrow

- `"string[pyarrow]"` falls back to NumPy
- `pd.ArrowDtype(pa.string())` raises.

---
## Opting into Arrow-backed DataFrames

Most I/O methods gained a new `dtype_backend` keyword to convert  the input data to Arrow datatypes during reading.

--
count: false

```python
>>> data = """a,b,c\n1,1.5,x\n,2.5,y"""

>>> df = pd.read_csv(StringIO(data), dtype_backend="pyarrow")
>>> df
      a    b  c
0     1  1.5  x
1  <NA>  2.5  y

>>> df.dtypes
a     int64[pyarrow]
b    double[pyarrow]
c    string[pyarrow]
dtype: object
```

--
count: false

`.convert_dtypes(dtype_backend="pyarrow")` can be used, if a function does not support the `dtype_backend` keyword yet.

---


## Speeding up I/O operations with PyArrow engine

Some I/O functions gained an `engine` keyword to parse the input with PyArrow.

- `read_csv` and `read_json` can dispatch to PyArrow readers.
- `read_parquet` and `read_orc` use PyArrow natively to read the input.

--
count: false

.larger[Benefits]

- Huge [performance improvements](https://datapythonista.me/blog/pandas-with-hundreds-of-millions-of-rows)
- Multithreading
- Zero-copy when using Arrow-backed DataFrames

---

## WARNING: Arrow support is still experimental

It is still early in adopting PyArrow dtypes and PyArrow-backed DataFrames.

--
count: false

.larger[This means:]

- The Arrow-backend is not yet supported everywhere.

  - Can lead to performance problems
  - Potential bugs
  - `GroupBy` and `merge` are popular examples.

--
count: false

- Potential upstream bugs in Arrow itself that aren't addressed yet.

---

## Roadmap for PyArrow support

--
count: false

* pandas will continue working on supporting PyArrow everywhere.

  This goal is reached when PyArrow-dtypes and NumPy-dtypes are both equally well supported.

  Ensure support for new dtypes that are available (Decimal, bytes, ...).

--
count: false

* Provide a way for users to opt into PyArrow-backed DataFrames globally.

---

# Non-nanosecond resolution in Timestamps

pandas only supported Timestamps in nanosecond resolution up until 2.0

--
count: false

.larger[&rarr;] Only dates between 1677 and 2262 can be represented

--
count: false

```python
>>> pd.Timestamp("1000-01-01")
OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1000-01-01 00:00:00

```

--
count: false

pandas 2.0 lifts this restriction!

--
count: false

Timestamps can be created in the following units:

- `seconds`
- `milliseconds`
- `microseconds`
---

## How to enable the new resolutions

Nearest resolution is inferred:

```python
>>> pd.Timestamp("1000-01-01")
Timestamp('1000-01-01 00:00:00')
```

--
count: false

Specify a higher precision to use a more accurate resolution:

```python
>>> pd.Timestamp("1000-01-01 10:00:00.123456")
Timestamp('1000-01-01 10:00:00.123456')
```

--
count: false

`as_unit` can be used to convert between units:

```python
>>> pd.Timestamp("1000-01-01").as_unit("us").unit
'us'
```

---

## Some caveats

--
count: false

- Non-nanosecond support is still new and is actively developed!

--
count: false

- Not every part of the API support non-nanosecond resolutions yet.

--
count: false

- Comparing two arrays with differing resolutions is still relatively slow.

    </textarea>
<!--    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>-->
    <script src="../remark-0.15.min.js" type="text/javascript">
    </script>
    <script>
	    remark.macros.scale = function (percentage) {
          var url = this;
          return '<img src="' + url + '" style="width: ' + percentage + '" />';
      };
      remark.macros.scaleH = function (percentage) {
          var url = this;
          return '<img src="' + url + '" style="height: ' + percentage + '" />';
      };
      config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9",
      };
      var slideshow = remark.create(config_remark);
    </script>
  </body>
</html>