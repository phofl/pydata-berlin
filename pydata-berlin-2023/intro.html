  <!--


-->

<!DOCTYPE html>
<html>
  <head>
    <title>pandas 2.0 and beyond</title>
    <meta charset="utf-8">
      <link rel="stylesheet" type="text/css" href="slides.css">
      <link rel="stylesheet" type="text/css" href="abs-layout.css" />
      <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css" />

<!--    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono';   }
      #slideshow .slide .content .cols.two .col { width: 48%; }
    </style>
-->
  </head>
  <body>
    <textarea id="source">

class: center, middle

![:scale 40%](img/pandas.svg)

# pandas 2.0 and beyond

### Copy-on-Write, Arrow-backed DataFrames and more

Joris Van den Bossche (Voltron Data), Patrick Hoefler (Coiled)<br>
PyConDE & PyData Berlin, April 17, 2023

https://github.com/jorisvandenbossche/<br>
[@jorisvdbossche](https://twitter.com/jorisvdbossche)/<br>
https://github.com/phofl/<br>


---

# About Joris

Joris Van den Bossche
<div style="margin-bottom:-20px"></div>

- Background: PhD bio-science engineer, air quality research
- Open source enthusiast: pandas core dev, geopandas maintainer, scikit-learn contributor
- Currently working part-time at Voltron Data on Apache Arrow

https://github.com/jorisvandenbossche   Twitter: [@jorisvdbossche](https://twitter.com/jorisvdbossche)


.affiliations[
  ![:scale 30%](img/apache-arrow.png)
  ![:scale 64%](img/voltrondata-logo-green.png)
]

---

# About Patrick

Patrick Hoefler
<div style="margin-bottom:-20px"></div>

- Background: M.Sc. in Mathematics
- Open source: pandas core dev
- Currently working at Coiled on Dask

https://github.com/phofl


.affiliations[
  ![:scale 20%](img/dask.png)
  ![:scale 30%](img/coiled.png)
]
---

class: center, middle

# pandas 2.0 and beyond

### Copy-on-Write, Arrow-backed DataFrames and more

---

count: false

# Arrow-backed DataFrames

Using PyArrow arrays to store the data of a DataFrame.


---

## ArrowDtype

`pd.ArrowDtype` or `f"{dtype}[pyarrow]"` creates Arrow-backed columns

--
count: false

```python
>>> import pyarrow as pa
>>> pd.Series([1, 2, 3], dtype=pd.ArrowDtype(pa.int64()))
0   1
1   2
2   3
dtype: int64[pyarrow]

>>> pd.Series([1, 2, 3], dtype="int64[pyarrow]")
0   1
1   2
2   3
dtype: int64[pyarrow]
```

--
count: false

These columns use the PyArrow memory layout and compute functionality.

---

## Dispatching to pyarrow.compute

The ExtensionArray interface of pandas dispatches to
[compute functions of PyArrow](https://arrow.apache.org/docs/python/compute.html).

```python
In [3]: ser = pd.Series(np.random.randint(1, 100, (5_000_000, )))
```
--
count: false
```python
In [4]: %timeit ser.unique()
10.6 ms ± 31.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
--
count: false

```python
In [5]: ser_arrow = ser.astype(pd.ArrowDtype(pa.int64()))

In [6]: %timeit ser_arrow.unique()
6.71 ms ± 6.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

--
count: false

.larger[&rarr;] PyArrow can provide a significant performance improvement

Not every method of pandas supports the compute functionality of PyArrow yet.

---

## PyArrow dtypes

PyArrow offers support for a wide variety of dtypes.

--
count: false

.larger[Benefits]

- Support of missing value indicators in every datatype

```python
>>> pd.Series([1, None], dtype="int64[pyarrow]")
0       1
1    <NA>
dtype: int64[pyarrow]
```
--
count: false
- An efficient string-datatype implementation

--
count: false

- Bytes, decimal, explicit null-datatype and [many more](https://arrow.apache.org/docs/python/api/datatypes.html#data-types-and-schemas).

---


## PyArrow string dtype

PyArrow offers fast and efficient in-memory string operations.

pandas implements PyArrow-based string operations through `"string[pyarrow]"` or `pd.ArrowDtype(pa.string())`.

--
count: false

These implementations provide

- significantly improved performance compared to NumPy's object dtype
- smaller memory footprint

--
count: false

If the functionality does not exist in PyArrow

- `"string[pyarrow]"` falls back to NumPy
- `pd.ArrowDtype(pa.string())` raises.

---
## Opting into Arrow-backed DataFrames

Most I/O methods gained a new `dtype_backend` keyword to convert  the input data to Arrow datatypes during reading.

--
count: false

```python
>>> data = """a,b,c\n1,1.5,x\n,2.5,y"""

>>> df = pd.read_csv(StringIO(data), dtype_backend="pyarrow")
>>> df
      a    b  c
0     1  1.5  x
1  <NA>  2.5  y

>>> df.dtypes
a     int64[pyarrow]
b    double[pyarrow]
c    string[pyarrow]
dtype: object
```

--
count: false

`.convert_dtypes(dtype_backend="pyarrow")` can be used, if a function does not support the `dtype_backend` keyword yet.

---


## Speeding up I/O operations with PyArrow engine

Some I/O functions gained an `engine` keyword to parse the input with PyArrow.

- `read_csv` and `read_json` can dispatch to PyArrow readers.
- `read_parquet` and `read_orc` use PyArrow natively to read the input.

--
count: false

.larger[Benefits]

- Huge [performance improvements](https://datapythonista.me/blog/pandas-with-hundreds-of-millions-of-rows)
- Multithreading
- Zero-copy when using Arrow-backed DataFrames

---

## WARNING: Arrow support is still experimental

It is still early in adopting PyArrow dtypes and PyArrow-backed DataFrames.

--
count: false

.larger[This means:]

- The Arrow-backend is not yet supported everywhere.

  - Can lead to performance problems
  - Potential bugs
  - `GroupBy` and `merge` are popular examples.

--
count: false

- Potential upstream bugs in Arrow itself that aren't addressed yet.

---

## Roadmap for PyArrow support

--
count: false

* pandas will continue working on supporting PyArrow everywhere.

  This goal is reached when PyArrow-dtypes and NumPy-dtypes are both equally well supported.

  Ensure support for new dtypes that are available (Decimal, bytes, ...).

--
count: false

* Provide a way for users to opt into PyArrow-backed DataFrames globally.

---

# Non-nanosecond resolution in Timestamps

pandas only supported Timestamps in nanosecond resolution up until 2.0

--
count: false

.larger[&rarr;] Only dates between 1677 and 2262 can be represented

--
count: false

```python
>>> pd.Timestamp("1000-01-01")
OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1000-01-01 00:00:00

```

--
count: false

pandas 2.0 lifts this restriction!

--
count: false

Timestamps can be created in the following units:

- `seconds`
- `milliseconds`
- `microseconds`
---

## How to enable the new resolutions

Nearest resolution is inferred:

```python
>>> pd.Timestamp("1000-01-01")
Timestamp('1000-01-01 00:00:00')
```

--
count: false

Specify a higher precision to use a more accurate resolution:

```python
>>> pd.Timestamp("1000-01-01 10:00:00.123456")
Timestamp('1000-01-01 10:00:00.123456')
```

--
count: false

`as_unit` can be used to convert between units:

```python
>>> pd.Timestamp("1000-01-01").as_unit("us").unit
'us'
```

---

## Some caveats

--
count: false

- Non-nanosecond support is still new and is actively developed!

--
count: false

- Not every part of the API support non-nanosecond resolutions yet.

--
count: false

- Comparing two arrays with differing resolutions is still relatively slow.


---

# Pandas Enhancement Proposals (PDEPs)

A PDEP is a proposal for a major change in pandas, in a similar way as a Python PEP or a NumPy NEP.

See "PDEP-1: Purpose and guidelines" for the details: https://pandas.pydata.org/pdeps/0001-purpose-and-guidelines.html



---

# PDEP-4: Consistent datetime parsing

Old behaviour: when not specifying a specific format, each value was being
parsed independently:

```python
>>> pd.to_datetime(['12-01-2000 00:00:00', '13-01-2000 00:00:00'])
DatetimeIndex([`'2000-12-01', '2000-01-13'`], dtype='datetime64[ns]', freq=None)
```

--
count: false

New behaviour: if not `format` is specified, the format will be guessed from the first string and applied to all values

```
>>> pd.to_datetime(['12-01-2000 00:00:00', '13-01-2000 00:00:00'])
...
ValueError: time data "13-01-2000 00:00:00" doesn't match format "%m-%d-%Y %H:%M:%S", at position 1. You might want to try:
    - passing \`format` if your strings have a consistent format;
    - passing \`format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;
    - passing \`format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.
```

--
count: false

Implemented in pandas 2.0! (full details: https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html)


---

# PDEP-8: In-place methods in pandas

Proposal to:

- The ``inplace`` parameter will be removed from any methods that never can be done inplace


    </textarea>
<!--    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>-->
    <script src="../remark-0.15.min.js" type="text/javascript">
    </script>
    <script>
	    remark.macros.scale = function (percentage) {
          var url = this;
          return '<img src="' + url + '" style="width: ' + percentage + '" />';
      };
      remark.macros.scaleH = function (percentage) {
          var url = this;
          return '<img src="' + url + '" style="height: ' + percentage + '" />';
      };
      config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9",
      };
      var slideshow = remark.create(config_remark);
    </script>
  </body>
</html>